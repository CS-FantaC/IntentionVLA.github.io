<!DOCTYPE HTML>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
    <title>IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</title>
    <meta charset="UTF-8">
    <meta name="author" content="Yandu Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- 补充Font Awesome图标库 -->
    <script src="https://kit.fontawesome.com/f30381e97f.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="lamp.css">
    <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;500;700&family=Noto+Sans:wght@400;500&family=Castoro:ital@0;1&display=swap" rel="stylesheet">
    <!-- 自定义基础样式 -->
    <style>
        /* 全局重置与基础样式 - 清除默认边框/背景 */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            border: none; /* 清除所有默认边框 */
            outline: none; /* 清除聚焦边框 */
        }
        body {
            font-family: 'Noto Sans', sans-serif;
            color: #333;
            background-color: #f9f9f9;
            line-height: 1.6;
        }
        h1, h2, h3, .section-name {
            font-family: 'Google Sans', sans-serif;
            font-weight: 700;
            color: #222;
        }
        a {
            text-decoration: none;
            transition: all 0.2s ease;
        }

        /* 容器样式 */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* 头部区域 - 作者/学校/按钮居中 */
        .header {
            padding: 40px 0 20px;
            text-align: center; /* 子元素整体居中 */
        }
        .header h1 {
            font-size: 2.2rem;
            margin-bottom: 25px;
            line-height: 1.3;
        }
        .authors {
            margin-bottom: 20px;
            font-size: 1.1rem;
        }
        .affiliations {
            margin-bottom: 25px;
            font-size: 0.95rem;
            color: #555;
        }
        .notes {
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 30px;
        }

        /* 按钮组 - 居中+无白色椭圆形（清除默认样式） */
        .btn-group {
            display: flex;
            flex-wrap: wrap;
            justify-content: center; /* 按钮组内部居中 */
            gap: 12px;
            margin-bottom: 15px;
            background: transparent; /* 清除容器背景 */
            padding: 0; /* 清除容器内边距 */
        }
        .link-button {
            display: inline-block;
            background: transparent; /* 清除按钮外层背景 */
        }
        .link-button-content {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 10px 20px;
            background-color: #444; /* 灰黑色背景（无白色边框/椭圆） */
            color: white !important; /* 白色文字 */
            border-radius: 8px; /* 圆角（非椭圆） */
            font-weight: 500;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.2s ease;
        }
        .link-button-content:hover {
            background-color: #222; /* hover时加深 */
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
            transform: translateY(-1px);
        }
        .link-button-content i {
            font-size: 1.1rem;
        }

        /* 内容区块样式 */
        .content-section {
            background-color: white;
            border-radius: 12px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            padding: 30px;
            margin-bottom: 35px;
        }
        .section-name {
            font-size: 1.7rem;
            margin-bottom: 25px;
            padding-bottom: 10px;
            border-bottom: 2px solid #f0f0f0;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        /* 媒体（视频/图片）- 视频居中，说明文字左对齐 */
        .media-container {
            margin-bottom: 25px;
            text-align: center; /* 视频/图片居中 */
        }
        .media-container video, .media-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
            margin-bottom: 15px; /* 视频与说明文字间距 */
            display: inline-block; /* 确保媒体元素响应式居中 */
        }
        .media-caption {
            font-size: 1.1rem;
            color: #666;
            max-width: 100%;
            text-align: left; /* 说明文字左对齐，保持阅读习惯 */
            padding: 0 5px;
        }
        .abs {
            font-size: 1.5rem;
            color: #666;
            max-width: 100%;
            text-align: left; /* 说明文字左对齐，保持阅读习惯 */
            padding: 0 5px;
        }

        /* 图片网格样式（多图展示） */
        .img-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 25px;
        }
        .img-grid img {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 6px rgba(0,0,0,0.08);
        }

        /* BibTex与联系区域 - 文本左对齐，BibTex留空 */
        .bibtex-section, .contact-section {
            margin-top: 40px;
        }
        .bibtex pre {
            background-color: #f5f5f5;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            min-height: 120px; /* 留空时保持容器高度 */
        }
        .contact-section p {
            font-size: 1rem;
            margin-top: 10px;
        }

        /* 访客计数器 - 强制居中（修复核心） */
        .visitor-counter {
            margin: 50px auto 30px; /* 上下边距+水平自动居中 */
            text-align: center; /* 内部图标和文字居中 */
            color: #777;
            font-size: 0.9rem;
            width: 100%; /* 占满父容器宽度，确保水平居中生效 */
            padding: 0 20px; /* 避免边缘紧贴 */
        }
        /* 确保计数器图标本身也居中 */
        .visitor-counter a, .visitor-counter img {
            display: inline-block; /* 消除inline元素默认间隙，确保居中对齐 */
            vertical-align: middle; /* 图标与文字垂直居中 */
        }
        .visitor-counter p {
            margin-top: 8px; /* 图标与文字间距 */
        }

        /* 响应式适配 */
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8rem;
            }
            .section-name {
                font-size: 1.5rem;
            }
            .content-section {
                padding: 20px;
            }
            .link-button-content {
                padding: 8px 16px;
                font-size: 0.9rem;
            }
            .img-grid {
                grid-template-columns: 1fr; /* 小屏单图排列 */
            }
            .visitor-counter {
                margin: 40px auto 20px; /* 小屏调整边距 */
            }
        }

        /* 平滑滚动 */
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>

<body>
    <!-- 头部区域 -->
    <div class="container">
        <div class="header">
            <h1>IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction</h1>
            
            <div class="authors">
                Yandu Chen<sup>1*</sup>, &nbsp;&nbsp;
                Kefan Gu<sup>2*</sup>, &nbsp;&nbsp;
                Yuqing Wen<sup>3*</sup>, &nbsp;&nbsp;
                Yucheng Zhao<sup>4&dagger;</sup>, &nbsp;&nbsp;
                Tiancai Wang<sup>4</sup>, &nbsp;&nbsp;
                Liqiang Nie<sup>1&Dagger;</sup>
            </div>

            <div class="affiliations">
                <sup>1</sup>Harbin Institute of Technology(Shenzhen), &nbsp;&nbsp;
                <sup>2</sup>Nanjing University, &nbsp;&nbsp;
                <sup>3</sup>University of Science and Technology of China, &nbsp;&nbsp;
                <sup>4</sup>Dexmal
            </div>

            <div class="notes">
                <sup>*</sup>This work was done during the internship at Dexmal. &nbsp;&nbsp;
                <sup>&dagger;</sup>Project lead. &nbsp;&nbsp;
                <sup>&Dagger;</sup>Corresponding Author.
            </div>

            <div class="btn-group">
                <span class="link-button">
                    <a class="link-button-content" href="" target="_blank">
                        <i class="fas fa-file-pdf"></i>
                        Paper
                    </a>
                </span>
                <span class="link-button">
                    <!-- <a class="link-button-content" href="" target="_blank"> -->
                    <a class="link-button-content" target="_blank">
                        <i class="fab fa-github"></i>
                        Github Repo
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content" href="#bib">
                        <i class="fas fa-bookmark"></i>
                        BibTex
                    </a>
                </span>
            </div>
        </div>

        <!-- 演示视频区域 - 视频居中 -->
        <div class="content-section">
            <div class="media-container">
                <h2 class="section-name">
                    <i class="fa-regular fa-circle-play"></i> Video
                </h2>
                <video controls autoplay muted loop poster="assets/video_cover.png" preload="metadata" style="max-width: 80%;">
                    <source src="assets/v1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p class="media-caption">
                    We propose the first Vision-Language-Action model that can ground implicit user intentions in cluttered environments, achieving superior performance against SOTA methods(&pi;<sub>0</sub>, ECoT) in real-world experiments and demonstrating strong generalization to unseen tasks.
                </p>
            </div>
        </div>

        <!-- Abstract -->
        <div class="content-section">
            <h2 class="section-name">
            <i class="fa-regular fa-feather-pointed"></i> Abstract
            </h2>
            <div class="media-container">
                <p class="abs">
                    Vision-Language-Action (VLA) models leverage pretrained vision-language models (VLMs) to couple perception with robotic control, offering a promising path toward general-purpose embodied intelligence. 
However, current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and reasoning-guided manipulation, these models are unable to perform implicit human intention reasoning required for complex, real-world interactions.
To overcome these limitations, we propose IntentionVLA, a VLA framework with a curriculum training paradigm and an efficient inference mechanism. 
Our proposed method first leverages carefully designed reasoning data that combine intention inference, spatial grounding, and compact embodied reasoning, endowing the model with both reasoning and perception capabilities. In the following finetuning stage,  IntentionVLA employs the compact reasoning outputs as contextual guidance for action generation, enabling  fast inference under indirect instructions.
Experimental results show that IntentionVLA substantially outperforms &pi;<sub>0</sub>, achieving 18% higher success rates with direct instructions and 28% higher than ECoT under intention instructions.
On out-of-distribution intention tasks, IntentionVLA achieves over twice the success rate of all baselines, and further enables zero-shot human-robot interaction with 40% success rate.
These results highlight IntentionVLA as a promising paradigm for next-generation human-robot interaction (HRI) systems.
                </p>
                <img src="assets/intro.png" alt="Motivation" style="max-width: 90%;">
            </div>
        </div>

        <!-- Motivation & Overview -->
        <!-- <div class="content-section">
            <h2 class="section-name">
            <i class="fa-regular fa-bullseye"></i> Motivation
            </h2>
            <div class="media-container">
                <img src="assets/motivation.png" alt="Motivation" style="max-width: 90%;">
                <p class="media-caption">
                    <strong>Problems with existing VLAs. </strong> Existing VLAs are finetuned to map explicit instructions to actionsoften fail to infer user intentions accurately and lack the ability to respond swiftly to dynamic environments. Given the instruction ''I want to call my friend'',  ECoT (right) reaches for the phone but infers slowly, while &pi;<sub>0</sub> (middle) misinterprets the instruction and grasps the rag. In contrast, our method (left) correctly infers the user's intention and enables rapid task completion.
                </p>
            </div>
        </div> -->

        <!-- 方法介绍区域 -->
        <div class="content-section">
            <h2 class="section-name">
                <i class="fas fa-cogs"></i> Method
            </h2>
            <!-- <h3 class="section-name">
                Model Architecture
            </h3> -->
            <div class="media-container">
                <img src="assets/model_arch.png" alt="IntentionVLA Architecture" style="max-width: 90%;">
                <p class="media-caption">
                    <strong>Overview of IntentionVLA.</strong> IntentionVLA achieves intention inference and reasoning-guided manipulation in one unified model. We first pretrain the VLM backbone with diverse intention reasoning data. Then we finetune the action module to decode action chunk which follows the compact reasoning output.
                </p>
            </div>
        </div>

        <!-- Pipeline -->
        <div class="content-section">
            <!-- <h3 class="section-name">
                <i class="fa-regular fa-diagram-project"></i> Reasoning Data Annotation Pipeline
            </h3> -->
            <div class="media-container">
                <img src="assets/pipeline.png" alt="Reasoning Data Annotation Pipeline" style="max-width: 90%;">
                <p class="media-caption">
                    <strong>Overview of our proposed reasoning data and efficient annotation pipeline.</strong> The pipeline consists of 4 decoupled modules that can run in parallel for high annotation efficiency. Intention and spatial reasoning chains are further compressed into compact short reasoning for fast inference.
                </p>
            </div>
        </div>

        <!-- Experiment -->
        <div class="content-section">
            <h2 class="section-name">
                <i class="fas fa-chart-line"></i> State-Of-The-Art Performance 
            </h2>
            <!-- In-Distribution Tasks -->
            <div class="media-container">
                <img src="assets/table1.png" alt="Performance on In-Distribution Tasks" style="max-width: 100%;">
                <p class="media-caption">
                    <strong>Performance on In-Distribution Tasks. </strong> IntentionVLA achieves the highest average
                    success rate on both direct instruction and intention instruction tasks.
                </p>
            </div>

            <h2 class="section-name" style="margin-top: 40px;">
                <i class="fas fa-check-circle"></i> Generalization Ability
            </h2>

            <!-- <h3 class="section-name" style="margin-top: 40px;">
                Performance on OOD Manipulation Tasks
            </h3> -->

            <div class="img-grid" style="
                display: grid;
                grid-template-columns: repeat(2, 1fr); /* 强制2列布局，确保左右排列 */
                gap: 20px; /* 保持图片间间距 */
                margin-bottom: 25px;
                align-items: center; /* 核心：让两张图片在垂直方向居中对齐（无视尺寸差异） */
                align-content: center; /* 辅助：当网格高度大于图片总高时，整体垂直居中 */
                min-height: 280px; /* 可选：设置最小高度，避免图片过矮时顶部紧贴标题（根据实际图片高度调整） */
            ">
                <!-- simpler.jpeg：保持原有宽高比，通过网格对齐自动向下调整至居中 -->
                <!-- <img src="assets/table1.png" alt="Performance on In-Distribution Tasks" style="
                    width: 100%;
                    height: auto; /* 不强制拉伸，保留原图比例 */
                    border-radius: 8px;
                    box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                "> -->
                <!-- calvin.jpeg：与simpler.jpeg垂直居中对齐 -->
                <img src="assets/unseen_ins.png" alt="Unseen Instructions Performance" style="
                width: 100%;
                height: auto;
                border-radius: 8px;
                box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                ">
                <img src="assets/table2_3.png" alt="Performance on OOD Tasks" style="
                    width: 100%;
                    height: auto;
                    border-radius: 8px;
                    box-shadow: 0 2px 6px rgba(0,0,0,0.08);
                ">
                <p class="media-caption">
                    <strong>Performance on OOD Manipulation Tasks.</strong> IntentionVLA generalizes beyond training data, understanding unseen intentions instructions, handling new objects with high precision, and responding quickly to human hand movements.
                     These results demonstrate that IntentionVLA enables robust, intention-aware, and real-time HRI in dynamic scenarios.
                </p>
            </div>

            <!-- <h3 class="section-name" style="margin-top: 40px;">
                Preserving Excellent Multimodal Reasoning Ability
            </h3> -->
            <div class="media-container">
                <img src="assets/Multimodal_bench.png" alt="Performance on Multimodal Benchmark" style="max-width: 60%;">
                <p class="media-caption">
                    <strong>Performance on Multimodal Benchmark.</strong> Benefiting from the diversity of our reasoning data and the effectiveness of the two-stage training paradigm, IntentionVLA preserves excellent mulimodal reasoning ability and consistently surpasses all VLA baselines, even
                    outperforms the strong MLLM LLaVA on three benchmarks.
                </p>
            </div>
        </div>

        <!-- BibTex与联系区域 - BibTex留空 -->
        <div class="content-section">
            <div class="bibtex-section" id="bib">
                <h2 class="section-name">BibTex</h2>
                <pre class="bibtex">
@article{
    @misc{wen2025lladavlavisionlanguagediffusion,
      title={IntentionVLA: Vision Language Diffusion Action Models}, 
      author={Yuqing Wen and Hebei Li and Kefan Gu and Yucheng Zhao and Tiancai Wang and Xiaoyan Sun},
      year={2025},
      eprint={2509.06932},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2509.06932}, 
}
}
                </pre>
            </div>

            <div class="contact-section">
                <h2 class="section-name">Contact</h2>
                <p>
                    Feel free to contact us at <strong>yanduchen77@gmail.com</strong>
                </p>
            </div>
        </div>

        <!-- 访客计数器 - 强制居中 -->
        <div class="visitor-counter">
            <a href="https://www.freecounterstat.com" title="web counter">
                <img src="https://counter1.optistats.ovh/private/freecounterstat.php?c=17hb8c4nryl5pwcns7euls8fcetl1lhu" border="0" alt="web counter">
            </a>
            <p>Visitor Count</p>
        </div>
    </div>

    <!-- 视频预加载优化JS -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const video = document.querySelector('video');
            if (video) {
                const observer = new IntersectionObserver((entries) => {
                    entries.forEach(entry => {
                        if (entry.isIntersecting) {
                            video.load();
                            observer.unobserve(video);
                        }
                    });
                });
                observer.observe(video);
            }
        });
    </script>
</body>
</html>